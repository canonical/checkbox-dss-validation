# TODO: add host-level Intel GPU checks (lspci, ls /dev/dri)

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/microk8s_install_intel_gpu_plugin_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'kubectl'
_summary: Install Intel K8s GPU Device Plugin
estimated_duration: 2m
command:
  set -e
  # Using kubectl directly due to this bug: https://github.com/canonical/microk8s/issues/4453
  kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/nfd?ref=v0.29.0'
  kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/nfd/overlays/node-feature-rules?ref=v0.29.0'
  kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/gpu_plugin/overlays/nfd_labeled_nodes?ref=v0.29.0'
  sleep 5
  kubectl -n node-feature-discovery rollout status ds/nfd-worker
  kubectl -n default rollout status ds/intel-gpu-plugin
  SLEEP_SECS=15
  echo "Info: sleeping for ${SLEEP_SECS} to allow pod status to update for subsequent tests."
  sleep ${SLEEP_SECS}
  echo "Test success: Intel K8s GPU Device Plugin deployed."

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/daemonset_check_name_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check DaemonSet Name
estimated_duration: 1s
command:
  set -e
  result=$(microk8s.kubectl get daemonset.apps -o jsonpath='{.items[0].metadata.name}')
  if [ "${result}" = "intel-gpu-plugin" ]; then
      echo "Test success: 'intel-gpu-plugin' daemonset is deployed!"
  else
      >&2 echo "Test failure: expected daemonset name 'intel-gpu-plugin' but got ${result}"
      exit 1
  fi

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/daemonset_check_number_available_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check number of available daemonsets
estimated_duration: 1s
command:
  set -e
  SLEEP_SECS=10
  RETRIES=10
  result=$(microk8s.kubectl get daemonset.apps -o jsonpath='{.items[0].status.numberAvailable}')
  retry_cnt=0
  while [ -z "${result}" ]; do
      if [ "${retry_cnt}" = "10" ]; then
          >&2 echo "Test failure: empty numberAvailable result after ${retry_cnt} retries."
          exit 1
      fi
      retry_cnt=$((retry_cnt+1))
      echo "Info: received empty numberAvailable result. Sleeping for ${SLEEP_SECS} seconds. Retry $retry_cnt/$RETRIES."
      sleep ${SLEEP_SECS}
      result=$(microk8s.kubectl get daemonset.apps -o jsonpath='{.items[0].status.numberAvailable}')
  done
  if [ "${result}" = "1" ]; then
      echo "Test success: 1 daemonset in numberAvailable status."
  else
      >&2 echo "Test failure: expected numberAvailable to be 1 but got ${result}"
      exit 1
  fi

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/daemonset_check_number_ready_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check number of ready daemonsets
estimated_duration: 1s
command:
  set -e
  SLEEP_SECS=10
  RETRIES=10
  result=$(microk8s.kubectl get daemonset.apps -o jsonpath='{.items[0].status.numberReady}')
  retry_cnt=0
  while [ -z "${result}" ]; do
      if [ "${retry_cnt}" = "10" ]; then
          >&2 echo "Test failure: empty numberReady result after ${retry_cnt} retries."
          exit 1
      fi
      retry_cnt=$((retry_cnt+1))
      echo "Info: received empty numberReady result. Sleeping for ${SLEEP_SECS} seconds. Retry $retry_cnt/$RETRIES."
      sleep ${SLEEP_SECS}
      result=$(microk8s.kubectl get daemonset.apps -o jsonpath='{.items[0].status.numberReady}')
  done
  if [ "${result}" = "1" ]; then
      echo "Test success: 1 daemonset in numberReady status."
  else
      >&2 echo "Test failure: expected numberReady to be 1 but got ${result}"
      exit 1
  fi

# TODO: add node label checks

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/dss_initialize_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Initialize DSS environment with common k8s manifests
estimated_duration: 2m
command:
  set -e
  DSS_COMMON_MANIFESTS=/tmp/data-science-stack/poc/common-manifests
  sudo microk8s.kubectl apply -f ${DSS_COMMON_MANIFESTS}/namespace.yaml
  sudo microk8s.kubectl apply -f ${DSS_COMMON_MANIFESTS}/mlflow.yaml
  sudo microk8s.kubectl apply -f ${DSS_COMMON_MANIFESTS}/notebooks.yaml
  sudo microk8s.kubectl wait --for condition=available --timeout 1200s -n dss deployment -l app=dss-mlflow
  echo "Test success: common DSS manifests deployed."

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/dss_check_service_name_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check Service Name
estimated_duration: 1s
command:
  set -e
  result=$(microk8s.kubectl get service -n dss -o jsonpath='{.items[0].metadata.name}')
  if [ "${result}" = "mlflow" ]; then
      echo "Test success: 'mlflow' service is deployed!"
  else
      >&2 echo "Test failure: expected service name 'mlflow' but got ${result}"
      exit 1
  fi

# TODO: check other service and deployment details

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/openvino_start_{{ driver }}
category_id: dss-regress
flags: simple
requires:
  executable.name == 'docker'
  executable.name == 'microk8s'
_summary: Launch OpenVINO+Jupyter container
estimated_duration: 6m
command:
  set -e
  DSS_INTEL_POC_ROOT=/tmp/data-science-stack/poc/intel
  OPENVINO_IMAGE="local/openvinotoolkit:render-addon"
  OPENVINO_TAR_IMAGE="/tmp/openvino.tar"
  echo "Building OpenVINO Docker image locally. Please be patient."
  docker build -q -t local/openvinotoolkit:mainbranch github.com/openvinotoolkit/openvino_notebooks
  render_gid=$(getent group render | cut -d: -f3)
  container_user=$(docker run --rm local/openvinotoolkit:mainbranch /usr/bin/whoami)
  cat ${DSS_INTEL_POC_ROOT}/openvino-image/Dockerfile | docker build -q \
    -t local/openvinotoolkit:render-addon \
    --build-arg render_gid=${render_gid} \
    --build-arg container_user=${container_user} -
  # hack as redirecting stdout anywhere but /dev/null throws a permission denied error
  # see: https://forum.snapcraft.io/t/eksctl-cannot-write-to-stdout/17254/4
  docker save local/openvinotoolkit:render-addon | tee ${OPENVINO_TAR_IMAGE} > /dev/null
  microk8s ctr image import ${OPENVINO_TAR_IMAGE}
  rm -f ${OPENVINO_TAR_IMAGE}
  sudo microk8s kubectl apply -f ${DSS_INTEL_POC_ROOT}/manifests/notebook-openvino.yaml
  sudo microk8s.kubectl wait --for condition=available --timeout 1200s -n dss deployment -l app=user-notebook-openvino
  echo "Test success: OpenVINO+Jupyter container is running."

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/openvino_deployment_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check OpenVINO Deployment
estimated_duration: 1s
command:
  set -e
  all_apps=$(microk8s.kubectl get deployment.apps -n dss -o jsonpath='{.items..metadata.name}')
  if grep -qw "notebook-openvino" <<<${all_apps}; then
      echo "Test success: 'notebook-openvino' app is deployed!"
  else
      >&2 echo "Test failure: expected result to contain app name 'notebook-openvino' but got ${result}"
      exit 1
  fi

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/openvino_libs_installed_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check jupyter and openvino python packages in OpenVINO container
estimated_duration: 1s
command:
  set -e
  pod=$(microk8s.kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}' | grep -o 'notebook-openvino.*')
  echo "Examining pod ${pod}..."
  microk8s.kubectl -n dss exec ${pod} -- pip show jupyter
  microk8s.kubectl -n dss exec ${pod} -- pip show openvino
  echo "Test success: jupyter and openvino python packages installed in container."

unit: template
template-resource: graphics_card
template-filter: graphics_card.driver in ['i915']
template-engine: jinja2
template-unit: job
id: intel_gpu_plugin/openvino_gpu_avail_{{ driver }}
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check OpenVINO GPU Availability
estimated_duration: 1s
command:
  set -e
  pod=$(microk8s.kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}' | grep -o 'notebook-openvino.*')
  echo "Examining pod ${pod}..."
  microk8s kubectl -n dss exec -i --tty=false ${pod} -- python3 \
  <<EOF
  import sys
  import openvino as ov
  core = ov.Core()
  if "GPU" in core.available_devices:
    full_device_name = core.get_property("GPU", "FULL_DEVICE_NAME")
    print(f"Test success: Intel GPU device ({full_device_name}) available to OpenVINO.")
    sys.exit(0)
  else:
    print("Test failure: No Intel GPU device available to OpenVINO.", file=sys.stderr)
    sys.exit(1)
  EOF

id: itex/check_itex_import
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check to see if itex can be imported
estimated_duration: 5m
command:
  TIMEOUT=1200s
  DSS_INTEL_MANIFESTS=/tmp/data-science-stack/poc/intel/manifests/
  launch_notebook() {
    local fullname=$1
    local app=$2
    echo -e "\n\$ dss start ${app} notebook"
    sudo microk8s kubectl apply -f ${DSS_INTEL_MANIFESTS}/notebook-${app}.yaml
    sudo microk8s.kubectl wait \
      --for condition=available \
      --timeout $TIMEOUT \
      -n dss\
      deployment \
      -l app=user-notebook-${fullname}
    echo -e "\n\$ dss status"
    MLFLOW_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      mlflow)
    NOTEBOOK_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      user-notebook-${fullname})
    echo "DSS has started successfully!"
  }
  remove_itex() {
    echo "Removing ITEX"
    sudo microk8s.kubectl delete -n dss deployment.apps/notebook-tensorflow
    sudo microk8s.kubectl delete -n dss service/user-notebook-tensorflow
  }
  launch_notebook tensorflow itex
  echo "Starting itex import test"
  pod=$(kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}' | grep -o 'notebook-tensorflow.*')
  echo "Found Tensorflow pod: ${pod}"
  sudo microk8s kubectl -n dss exec ${pod} -- python3 -c "import intel_extension_for_tensorflow as itex; import tensorflow; import jupyter"
  if [ "$?" = 0 ]
  then
    echo "PASS: Found module"
    remove_itex
    exit 0
  else
    >&2 echo "FAIL: Did not find ITEX python module"
    remove_itex
    exit 1
  fi

id: ipex/check_ipex_import
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check to see if ipex can be imported
estimated_duration: 5m
command:
  TIMEOUT=1200s
  DSS_INTEL_MANIFESTS=/tmp/data-science-stack/poc/intel/manifests/
  launch_notebook() {
    local fullname=$1
    local app=$2
    echo -e "\n\$ dss start ${app} notebook"
    sudo microk8s kubectl apply -f ${DSS_INTEL_MANIFESTS}/notebook-${app}.yaml
    echo -e "\n\$ dss ${app} manifest applied"
    sudo microk8s.kubectl wait \
      --for condition=available \
      --timeout $TIMEOUT \
      -n dss\
      deployment \
      -l app=user-notebook-${fullname} > /tmp/ipex_log.txt
    echo -e "\n\$ dss status"
    MLFLOW_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      mlflow)
    NOTEBOOK_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      user-notebook-${fullname})
    echo "DSS has started successfully!"
  }
  remove_ipex() {
    echo "Removing IPEX"
    sudo microk8s.kubectl delete -n dss deployment.apps/notebook-pytorch
    sudo microk8s.kubectl delete -n dss service/user-notebook-pytorch
  }
  launch_notebook pytorch ipex
  echo "Starting ipex import test"
  pod=$(kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}' | grep -o 'notebook-pytorch.*')
  echo "Found PyTorch pod: ${pod}"
  sudo microk8s kubectl -n dss exec ${pod} -- python3 -c "import intel_extension_for_pytorch as ipex; import torch; import jupyter"
  if [ "$?" = 0 ]
  then
    echo "PASS: Found module"
    remove_ipex
    exit 0
  else
    >&2 echo "FAIL: Did not find IPEX python module"
    remove_ipex
    exit 1
  fi

id: ipex/ipex_gpu_avial
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check IPEX GPU Availability
estimated_duration: 5m
command:
  set -e
  TIMEOUT=1200s
  DSS_INTEL_MANIFESTS=/tmp/data-science-stack/poc/intel/manifests/
  launch_notebook() {
    local fullname=$1
    local app=$2
    echo -e "\n\$ dss start ${app} notebook"
    sudo microk8s kubectl apply -f ${DSS_INTEL_MANIFESTS}/notebook-${app}.yaml
    echo -e "\n\$ dss ${app} manifest applied"
    sudo microk8s.kubectl wait \
      --for condition=available \
      --timeout $TIMEOUT \
      -n dss\
      deployment \
      -l app=user-notebook-${fullname} > /tmp/ipex_log.txt
    echo -e "\n\$ dss status"
    MLFLOW_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      mlflow)
    NOTEBOOK_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      user-notebook-${fullname})
    echo "DSS has started successfully!"
  }
  remove_ipex() {
    echo "Removing IPEX"
    sudo microk8s.kubectl delete -n dss deployment.apps/notebook-pytorch
    sudo microk8s.kubectl delete -n dss service/user-notebook-pytorch
  }
  launch_notebook pytorch ipex
  echo "Starting ipex import test"
  kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}'
  pod=$(kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}' | grep -o 'notebook-pytorch.*')
  echo "Found PyTorch pod: ${pod}"
  gpu_grep_out=$(sudo microk8s kubectl -n dss exec ${pod} -- python3 -c '
  import sys
  import torch
  import intel_extension_for_pytorch as ipex
  print(torch.__version__)
  print(ipex.__version__)
  try:
    [print(f"[{i}]: {torch.xpu.get_device_properties(i)}") for i in range(torch.xpu.device_count())];
    sys.exit(0)
  except Exception:
    print("Encountered an error getting XPU device properties")
    sys.exit(1)
  ' | grep "dev_type=.gpu" 2>&1)
  remove_ipex
  if [[ -z ${gpu_grep_out} ]]; then
    echo "FAIL: No GPU found"
    exit 1
  elif [[ $gpu_grep_out =~ "gpu" ]]; then
    echo "PASS: GPU found"
    exit 0
  else
    echo "FAIL: Unexpected result"
    echo ${gpu_grep_out}
    exit 1
  fi

id: itex/itex_gpu_avail
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check ITEX GPU Availability
estimated_duration: 5m
command:
  TIMEOUT=1200s
  DSS_INTEL_MANIFESTS=/tmp/data-science-stack/poc/intel/manifests/
  launch_notebook() {
    local fullname=$1
    local app=$2
    echo -e "\n\$ dss start ${app} notebook"
    sudo microk8s kubectl apply -f ${DSS_INTEL_MANIFESTS}/notebook-${app}.yaml
    sudo microk8s.kubectl wait \
      --for condition=available \
      --timeout $TIMEOUT \
      -n dss\
      deployment \
      -l app=user-notebook-${fullname}
    echo -e "\n\$ dss status"
    MLFLOW_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      mlflow)
    NOTEBOOK_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      user-notebook-${fullname})
    echo "DSS has started successfully!"
  }
  remove_itex() {
    echo "Removing ITEX"
    sudo microk8s.kubectl delete -n dss deployment.apps/notebook-tensorflow
    sudo microk8s.kubectl delete -n dss service/user-notebook-tensorflow
  }
  launch_notebook tensorflow itex
  echo "Starting itex GPU check test"
  pod=$(kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}' | grep -o 'notebook-tensorflow.*')
  echo "Found Tensorflow pod: ${pod}"
  gpu_grep_out=$(sudo microk8s kubectl -n dss exec ${pod} -- python3 -c '
  import tensorflow as tf
  ' 2>&1 >/dev/null | grep "Intel Extension for Tensorflow\* GPU backend is loaded")
  remove_itex
  if [[ -z ${gpu_grep_out} ]]; then
    echo "ERROR: No GPU found"
    exit 1
  elif [[ $gpu_grep_out =~ "GPU backend is loaded" ]]; then
    echo "PASS: GPU found"
    exit 0
  else
    echo "FAIL: Unexpected result"
    exit 1
  fi

id: itex/itex_inference
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check ITEX Inference
estimated_duration: 5m
command:
  TIMEOUT=1200s
  DSS_INTEL_MANIFESTS=/tmp/data-science-stack/poc/intel/manifests/
  launch_notebook() {
    local fullname=$1
    local app=$2
    echo -e "\n\$ dss start ${app} notebook"
    sudo microk8s kubectl apply -f ${DSS_INTEL_MANIFESTS}/notebook-${app}.yaml
    sudo microk8s.kubectl wait \
      --for condition=available \
      --timeout $TIMEOUT \
      -n dss\
      deployment \
      -l app=user-notebook-${fullname}
    echo -e "\n\$ dss status"
    MLFLOW_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      mlflow)
    NOTEBOOK_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      user-notebook-${fullname})
    echo "DSS has started successfully!"
  }
  remove_itex() {
    echo "Removing ITEX"
    sudo microk8s.kubectl delete -n dss deployment.apps/notebook-tensorflow
    sudo microk8s.kubectl delete -n dss service/user-notebook-tensorflow
  }
  launch_notebook tensorflow itex
  echo "Starting itex GPU check test"
  pod=$(kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}' | grep -o 'notebook-tensorflow.*')
  echo "Found Tensorflow pod: ${pod}"
  sudo microk8s kubectl -n dss exec ${pod} -- pip3 install "tensorflow_hub==0.15.0" pillow
  #sudo microk8s kubectl -n dss exec ${pod} -- python3 -c '
  gpu_grep_out=$(sudo microk8s kubectl -n dss exec ${pod} -- python3 -c '
  import tensorflow as tf
  print("Tensorflow version {}".format(tf.__version__))
  import tensorflow_hub as hub
  from tensorflow.keras.applications.imagenet_utils import preprocess_input, decode_predictions
  from tensorflow.keras.preprocessing import image
  import numpy as np
  import urllib
  import os
  import sys
  def download(url, filename):
      with urllib.request.urlopen(url) as response, open(filename, "wb") as out_file:
          data = response.read()
          out_file.write(data)
  def download_img():
      ImageURL =  "https://github.com/intel/caffe/raw/master/examples/images/"
      image_names = ["cat.jpg"]
      for name in image_names:
          url = ImageURL + name
          if not os.path.exists(name):
              download(url, name)
              print("Downloaded {}".format(name))
  def load_data(orig):
      img_width, img_height = 224, 224
      print("Load %s to inference\n" % orig)
      img = image.load_img(orig, target_size=(img_width, img_height))
      x = image.img_to_array(img)
      x = np.expand_dims(x, axis=0)
      x = preprocess_input(x, mode="tf")
      x= x/2+ 0.5
      return x
  def main():
      NUM_ITERATIONS = 500
      module = hub.KerasLayer("https://tfhub.dev/google/supcon/resnet_v1_50/imagenet/classification/1")
      download_img()
      images = load_data("cat.jpg")
      try:
          for i in range(NUM_ITERATIONS):
              logits = module(images)
              logits = tf.nn.softmax(logits)
              logits = logits.numpy()
              model_index = decode_predictions(logits, top=1)[0]
          print("PASS: No inference errors")
          sys.exit(0)
      except Exception as e:
          print("FAILED: An error occured")
          print(e)
          sys.exit(1)
  main()
  ')
  remove_itex
  if [[ -z ${gpu_grep_out} ]]; then
    >&2 echo "FAIL: Inference Failed"
    exit 1
  elif [[ $gpu_grep_out =~ "PASS: No inference errors" ]]; then
    echo "PASS: Inference Succeeded"
    exit 0
  else
    >&2 echo "FAIL: Unexpected result"
    exit 1
  fi

id: ipex/ipex_train
category_id: dss-regress
flags: simple
requires: executable.name == 'microk8s'
_summary: Check ITEX Inference
estimated_duration: 5m
command:
  set -e
  TIMEOUT=1200s
  DSS_INTEL_MANIFESTS=/tmp/data-science-stack/poc/intel/manifests/
  launch_notebook() {
    local fullname=$1
    local app=$2
    echo -e "\n\$ dss start ${app} notebook"
    sudo microk8s kubectl apply -f ${DSS_INTEL_MANIFESTS}/notebook-${app}.yaml
    echo -e "\n\$ dss ${app} manifest applied"
    sudo microk8s.kubectl wait \
      --for condition=available \
      --timeout $TIMEOUT \
      -n dss\
      deployment \
      -l app=user-notebook-${fullname} > /tmp/ipex_log.txt
    echo -e "\n\$ dss status"
    MLFLOW_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      mlflow)
    NOTEBOOK_IP=$(sudo microk8s.kubectl get svc \
      -n dss \
      -o jsonpath="{.spec.clusterIP}" \
      user-notebook-${fullname})
    echo "DSS has started successfully!"
  }
  remove_ipex() {
    echo "Removing IPEX"
    sudo microk8s.kubectl delete -n dss deployment.apps/notebook-pytorch
    sudo microk8s.kubectl delete -n dss service/user-notebook-pytorch
  }
  launch_notebook pytorch ipex
  echo "Starting ipex import test"
  kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}'
  pod=$(kubectl get pods -n dss -o=jsonpath='{.items..metadata.name}' | grep -o 'notebook-pytorch.*')
  echo "Found PyTorch pod: ${pod}"
  #sudo microk8s kubectl -n dss exec ${pod} -- python3 -c '
  #gpu_grep_out=$(sudo microk8s kubectl -n dss exec ${pod} -- python3 -c '
  sudo microk8s kubectl -n dss exec ${pod} -- python3 -c '
  import torch
  import torchvision
  import intel_extension_for_pytorch as ipex
  LR = 0.001
  DOWNLOAD = True
  DATA = "datasets/cifar10/"
  transform = torchvision.transforms.Compose(
      [
          torchvision.transforms.Resize((224, 224)),
          torchvision.transforms.ToTensor(),
          torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
      ]
  )
  train_dataset = torchvision.datasets.CIFAR10(
      root=DATA,
      train=True,
      transform=transform,
      download=DOWNLOAD,
  )
  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)
  model = torchvision.models.resnet50()
  criterion = torch.nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
  model.train()
  model = model.to("xpu")
  criterion = criterion.to("xpu")
  model, optimizer = ipex.optimize(model, optimizer=optimizer)
  for batch_idx, (data, target) in enumerate(train_loader):
      data = data.to("xpu")
      target = target.to("xpu")
      optimizer.zero_grad()
      output = model(data)
      loss = criterion(output, target)
      loss.backward()
      optimizer.step()
      print(batch_idx)
  torch.save(
      {
          "model_state_dict": model.state_dict(),
          "optimizer_state_dict": optimizer.state_dict(),
      },
      "checkpoint.pth",
  )
  print("Execution finished")
  '
  #')
  remove_ipex
  if [[ -z ${gpu_grep_out} ]]; then
    >&2 echo "FAIL: Training Failed"
    exit 1
  elif [[ $gpu_grep_out =~ "PASS: No inference errors" ]]; then
    echo "PASS: Training Succeeded"
    exit 0
  else
    >&2 echo "FAIL: Unexpected result"
    exit 1
  fi